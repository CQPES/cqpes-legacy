# CQPES Legacy

This package is a Python implementation of potential energy surface (PES) fitting with permutational invariant polynomials neural network (PIP-NN).

## Installation

Here is an environment file `env.yml` inside the main directory, simply use the command below to create a conda virtual environment.

```bash
$ conda env create -n cqpes-env -f env.yml
```

## Usage

Before starting the tutorial, make sure the virtual environment has been activated.

```bash
$ conda activate cqpes-env
```

### 1. Generate PIP basis

To remove the barrier of generating PIP basis, a wrapper of MSA-2.0 by Bowman's group is available. First, clone the repo and get into the directory.

```bash
$ git clone https://github.com/CQPES/PyMSA-Builder.git
$ cd PyMSA-Builder
```

The structure of the directory looks like

```bash
$ tree
.
├── build.py
├── LICENSE
├── README.md
└── src
    ├── derivative_c.pl
    ├── derivative.pl
    ├── postemsa_c.pl
    └── postemsa.pl

1 directory, 7 files
```

Then type this command to generate the PIP basis via interactive options

```bash
$ python3 build.py
Cloning into 'MSA-2.0'...
remote: Enumerating objects: 141, done.
remote: Counting objects: 100% (94/94), done.
remote: Compressing objects: 100% (91/91), done.
remote: Total 141 (delta 52), reused 7 (delta 3), pack-reused 47
Receiving objects: 100% (141/141), 188.23 KiB | 606.00 KiB/s, done.
Resolving deltas: 100% (67/67), done.
--------------------------------------------------------------------------------
Building executable `msa`...
rm -rf *~  *.o  msa 
g++ -O2 -Wall -c msa.cpp -I ./header;
g++ -O2 -Wall -c monomial.cpp -I ./header;
g++ -O2 -Wall -c polynomial.cpp -I ./header;
g++ -O2 -Wall -o msa msa.o monomial.o polynomial.o -I ./header; 
--------------------------------------------------------------------------------
Build Python module? [Y/n]
Generate C code? [Y/n]
Build settings:
- Fortran code will be generated by default.
- Python module will be built via `f2py`.
--------------------------------------------------------------------------------
```

Type `Y` then `enter`, or `enter` to build a Python module, then `n` for C code since this has not been implemented yet. 

```bash
Molecule configuration: 4 1
Max degree of PIP: 4
Range parameter alpha for Morse-like variable (default 2.0): 1.0
PIP settings:
- Molecule configuration: 4 1
- Max degree: 4
- Range parameter alpha: 1.0
--------------------------------------------------------------------------------
```

In this example, we will train the PES of  H<sub>2</sub> + H<sub>2</sub>O system, the order of atoms in the raw data `xyz` file is `H H H H O`, i.e. A<sub>4</sub>B. The max degree of PIP should be at least greater or equal than 4. The range parameter $alpha$ determines the long-range fitting ability and 1.0 Angstrom is enough.

```bash
Generating PIP basis...
Generating Fortran code...
Building Python module...
To test Python module `msa`, run command:
$ python3 -c "from msa import basis, gradient;print(basis.__doc__); print(gradient.__doc__)"
Please check the output if any error exists!
--------------------------------------------------------------------------------
Done!
```

If there is nothing wrong, PIP basis will be generated and Python module will be built. 

```bash
$ tree -L 2
.
├── basis
│   ├── MOL_4_1_4.BAS
│   ├── MOL_4_1_4.FOC
│   ├── MOL_4_1_4.MAP
│   ├── MOL_4_1_4.MONO
│   └── MOL_4_1_4.POLY
├── build.py
├── fortran
│   ├── basis.f90
│   ├── gradient.f90
│   └── msa.pyf
├── LICENSE
├── MSA-2.0
│   ├── geom.inp
│   ├── msa.py
│   ├── param.inp
│   ├── README.md
│   ├── reset
│   ├── src
│   └── Tutorial.txt
├── python
│   └── msa.cpython-39-x86_64-linux-gnu.so
├── README.md
└── src
    ├── derivative_c.pl
    ├── derivative.pl
    ├── postemsa_c.pl
    └── postemsa.pl

6 directories, 22 files
```

The PIP basis of A<sub>4</sub>B system can be found in `fortran` directory and the Python wrapper module is in `python` directory. 

Remember to copy the built Python library (be like `msa.cpython-310-x86_64-linux-gnu.so`) to `src/` folder if you want to try this tutorial.

### 2. Prepare data

The data for training PES should be stored in standard `xyz` format. An `xyz` file can contains several frames of molecules, and for each frame,

1. A line containing the number of atoms
2. A comment line, which can be used to store the energy of current configuration, in **Hartree**
3. The Cartesian coordinates of current configuration, in **Angstrom**

```
5

C	          0.00000000           0.00000000           0.00000000
H	          1.08594333           0.00000000           0.00000000
H	         -0.36198111           0.51191859          -0.88666901
H	         -0.36198111          -1.02383719           0.00000000
H	         -0.36198111           0.51191859           0.88666901
```

For the potential energy, simply write the data into a plain text file. Make sure there is only one piece of energy data in each line, and the potential energy should be in **Hartree**.

Here exists a dataset of CH<sub>4</sub> molecule in directory [`rawdata`](https://github.com/CQPES/cqpes-legacy/tree/main/rawdata).

To prepare the data for training, configurations should be given in a json file like [`config/prepare.json`](https://github.com/CQPES/cqpes-legacy/blob/main/config/prepare.json).

```json
{
    "xyz": "/home/jhli/CQPES-legacy/rawdata/CH4.xyz",
    "energy": "/home/jhli/CQPES-legacy/rawdata/CH4_CCSD-T.dat",
    "ref_energy": null,
    "alpha": 1.0,
    "output": "/home/jhli/CQPES-legacy/data"
}
```

The `xyz` and `energy` has been mentioned before.

For `ref_energy`, the potential energy `V` (in eV) will be calculated by

```
V = (E - E_ref) * 27.2107
```

Usually, for mono molecular system, `ref_energy` can be the energy of stationary point, while for reaction / interaction system, `ref_energy` can be the energy of all species separated far enough.

`alpha` is the range parameter in Morse-like transformation, commonly, it should be 1.0.

`output` indicates the directory to store pre-processed data, in which all data will be stored in numpy `*.npy` format.

Run `./prepare.sh` to prepare the data by your own.

### 3. Train a PES

In previous section, the molecules in `xyz` format with the energy has been packed as numpy `npy` file. In this section, a PES of the system will be trained.

Here is a configuration file `config/train.json` defines the details in training.

```json
{
    "data": "/home/jhli/CQPES-legacy/data",
    "network": {
        "layers": [
            20,
            20
        ],
        "activation": "tanh"
    },
    "fit": {
        "lr": 0.5,
        "epoch": 1000,
        "batch_size": -1
    },
    "lm": {
        "adaptive_scaling": true,
        "fletcher": false,
        "solve_method": "solve",
        "jacobian_max_num_rows": 500
    },
    "split": [
        0.9,
        0.05,
        0.05
    ],
    "workdir": "model"
}
```

The `data` section is the path to previous packed data. The `network` section defines the number and size of hidden layers with the activation function, commonly, 2 hidden layers and `tanh` activation function will work. The `fit` section contains hyper-parameters in training, `lr` is the learning rate (0.1 ~ 1.0), `epoch` is the number of epochs to train (500~2000), `batch_size` is the size of mini-batch in traning. However, PIP-NN trained with second order optimizers, especially Levenberg-Marquardt (LM), will show better performance than first order optimizers. Thus, the hyper-parameters are tuned for LM optimizer, which requires more data in one batch to estimate better Hessian matrix. Setting `batch_size` as `-1` means full-batch training. Options in `lm` secitons controls the LM optimizer, for more details, refer to the [`DampingAlgorithm`](https://github.com/CQPES/cqpes-legacy/blob/d715aa1814a67c0a3cdbb8a67a1ff59a6b4472c5/src/train/levenberg_marquardt.py#L197). `split` is the ratio of each subset, training, validation, and test. `workdir` indicates the directory to save trained parameters and logs.

To start training, run `train.sh` and the results will be outputed in the `workdir` with an extra timestamp, like [`model-2024-06-29 16:24:34.294319`](https://github.com/CQPES/cqpes-legacy/tree/main/model-2024-06-29%2016%3A24%3A34.294319).

### 4. Evaluation

To evaluate a trained PES, a Jupyter notebook has been provided in [`src/train/evaluate.ipynb`](https://github.com/CQPES/cqpes-legacy/blob/main/src/train/evaluate.ipynb). Before running this notebook, remember to fill the path to data directory and training output directory.

```python3

config_json = "/home/jhli/CQPES-legacy/config/train.json"
workdir = "/home/jhli/CQPES-legacy/model-2024-06-29 16:24:34.294319"
```

After running, the max absolute error, mean absolute error (MAE), mean squared error (MSE), and root mean squared error (RMSE) will be calculate and printed as below (in `meV`).

|   | Dataset | MAE      | MSE      | RMSE     |
| - | ------- | -------- | -------- | -------- |
| 0 | train   | 0.334654 | 0.702226 | 0.837989 |
| 1 | valid   | 0.371523 | 1.407177 | 1.186245 |
| 2 | test    | 0.339292 | 0.829823 | 0.910946 |
| 3 | total   | 0.336729 | 0.336729 | 0.580283 |

The error distribution will be plotted in a scattering figure.

### 5. Export a model

After training several models, one may wonder how to use the best model in later simulations. The checkpoints saved during training only contains the parameters, lacking in the architecture. When exporting a model, the model will be built and the parameters will be loaded, then this model can be saved in `h5` or `potfit` format.

- The `h5` format is the standard format for saving a Keras model, one can load the exported `h5` file to create the NN part in PIP-NN.

- The `potfit` format can store the architecture and parameters of a model in plain text format, which is recommanded when using PIP-NN with Fortran code. In previous work by Jun Li's group, the PIP-NN models were saved in this format.

Here is a configuration file `config/export.json` defines the details in exporting a model.

```json
{
    "train_config": "/home/jhli/Developer/cqpes-legacy/config/train.json",
    "ckpt": "/home/jhli/Developer/cqpes-legacy/model-2024-06-29 16:24:34.294319/ckpt/model-epoch-1000-val-mse-1.44331e-07.h5",
    "output": "/home/jhli/Developer/cqpes-legacy/model-2024-06-29 16:24:34.294319"
}
```

The `train_config` is the path to configuration file used in training, which can be used to construct the model. The `ckpt` indicates the parameters. The `output` is the directory to save exported model.

To export `h5` model

```bash
$ python3 -u src/export/export.py -c config/export.json -t h5 
Exporting model in h5 format...
h5 file saved to /home/jhli/Developer/cqpes-legacy/model-2024-06-29 16:24:34.294319/model.h5
Model exported successfully!
```

To export `potfit` model

```bash
Exporting model in potfit format...
weights file saved to /home/jhli/Developer/cqpes-legacy/model-2024-06-29 16:24:34.294319/weights.txt
biases  file saved to /home/jhli/Developer/cqpes-legacy/model-2024-06-29 16:24:34.294319/biases.txt
Model exported successfully!
```

The exported models are sufficient for examples in [`interface`](https://github.com/CQPES/cqpes-legacy/tree/main/interface) directory.

### 6. Interfaces

## Reference
